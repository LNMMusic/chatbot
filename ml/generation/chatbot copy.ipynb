{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA CONTROLLERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters\n",
    "class Dataset:\n",
    "    # constructor\n",
    "    def __init__(self, text:list[str]):\n",
    "        self.text:list[str] = text  \n",
    "\n",
    "    # methods\n",
    "    # -> lower: convert text to lowercase\n",
    "    def Filter(self):\n",
    "        # convert text to lowercase\n",
    "        self.text = [t.lower() for t in self.text]\n",
    "        # remove punctuation except tabs and alphabetics and numbers\n",
    "        self.text = [re.sub(r'[^a-z0-9\\s\\t]', '', t) for t in self.text]\n",
    "\n",
    "    # -> chars: remove punctuation except alphabetics and return sorted list of chars_set\n",
    "    def Chars(self) -> list[str]:\n",
    "        # remove punctuation expect alphabetics and numbers\n",
    "        text = [re.sub(r'[^a-z0-9]', '', t) for t in self.text]\n",
    "\n",
    "        # return sorted list of chars_set\n",
    "        chars = [' ']\n",
    "        for c in ''.join(text):\n",
    "            chars.append(c)\n",
    "            \n",
    "        vocab = sorted(list(set(chars)))\n",
    "        return vocab\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VOCAB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    # constructor\n",
    "    def __init__(self, chars_set:list[str]):\n",
    "        # attributes\n",
    "        # -> vocab: represents all the chars in the dataset\n",
    "        # -> size: represents the size of the vocab\n",
    "        self.vocab:list[str] = chars_set\n",
    "        self.size:int        = len(self.vocab)\n",
    "\n",
    "        # -> mapper: maps each char to an index in the vocab and vice versa\n",
    "        self.chars_ix:dict[str,int] = {c:i for i, c in enumerate(self.vocab)}\n",
    "        self.ix_chars:dict[int,str] = {i:c for i, c in enumerate(self.vocab)}\n",
    "\n",
    "    # methods\n",
    "    # -> encode: encodes a char into an index\n",
    "    # -> decode: decodes an index into a char\n",
    "    def encode(self, char:str) -> int:\n",
    "        return self.chars_ix[char]\n",
    "    def decode(self, ix:int) -> str:\n",
    "        return self.ix_chars[ix]\n",
    "\n",
    "    # -> hot_encode: encodes a char into a one-hot vector with size of the vocab\n",
    "    #                (matrix of zeros with a 1 in the index of the char)\n",
    "    # -> hot_decode: decodes a one-hot vector into a char\n",
    "    def hot_encode(self, char:str) -> np.ndarray:\n",
    "        # create matrix of zeros with shape (vocab_size)\n",
    "        arr = np.zeros(self.size)\n",
    "\n",
    "        # set the ix of the char\n",
    "        arr[self.encode(char)] = 1\n",
    "        return arr\n",
    "\n",
    "    def hot_decode(self, data:np.ndarray) -> str:\n",
    "        # get the index of the max value\n",
    "        max_arg_ix = np.argmax(data)\n",
    "\n",
    "        # get the char from the index\n",
    "        return self.decode(max_arg_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phrase:\n",
    "    # constructor\n",
    "    def __init__(self, vocab:Vocab, max_len:int):\n",
    "        # attributes\n",
    "        # -> vocab: represents the vocab used to encode and decode the phrase\n",
    "        # -> max_len: represents the max length of the phrase (this for standardization)\n",
    "        self.vocab:Vocab = vocab\n",
    "        self.max_len:int = max_len\n",
    "\n",
    "    # methods\n",
    "    # -> encode: encodes a phrase into a matrix of one-hot vectors\n",
    "    # -> decode: decodes a matrix of one-hot vectors into a phrase\n",
    "    def encode(self, phrase:str) -> np.ndarray:\n",
    "        # create matrix of zeros with shape (max_len, vocab_size)\n",
    "        arr = np.zeros((self.max_len, self.vocab.size))\n",
    "\n",
    "        # iterate over the phrase\n",
    "        for i, c in enumerate(phrase):\n",
    "            if i >= self.max_len:\n",
    "                break\n",
    "\n",
    "            # encode the char into a one-hot vector\n",
    "            char = self.vocab.hot_encode(c)\n",
    "\n",
    "            # set the one-hot vector in the matrix\n",
    "            arr[i] = char\n",
    "\n",
    "        return arr\n",
    "\n",
    "    def decode(self, data:np.ndarray) -> str:\n",
    "        # create an empty string\n",
    "        phrase = ''\n",
    "\n",
    "        # iterate over the matrix\n",
    "        for v in data:\n",
    "            # decode the one-hot vector into a char\n",
    "            char = self.vocab.hot_decode(v)\n",
    "\n",
    "            # add the char to the phrase\n",
    "            phrase += char\n",
    "\n",
    "        return phrase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INITIALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "PHRASE_MAX_LEN = 50\n",
    "\n",
    "# _________________________________________________________________\n",
    "# Dataset\n",
    "with open(\"dialogs.txt\", \"r\") as f:\n",
    "    text = f.readlines()\n",
    "\n",
    "dataset = Dataset(text)\n",
    "dataset.Filter()\n",
    "\n",
    "\n",
    "# _________________________________________________________________\n",
    "# App\n",
    "# -> vocab: controller that handles the encoding and decoding of chars\n",
    "# -> phraser: controller that handles the encoding and decoding of phrases\n",
    "chars_set = dataset.Chars()\n",
    "\n",
    "vocab = Vocab(chars_set)\n",
    "phraser = Phrase(vocab, PHRASE_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi how are you doing\\tim fine how about yourself\\n'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hot encode a simple phrase (string -> list of chars -> list of ix -> matrix)\n",
    "phrase = \"hello\"\n",
    "\n",
    "# create a phrase encoded\n",
    "arr = phraser.encode(phrase)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello                                             '"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hot decode a simple phrase (matrix -> list of ix -> list of chars -> string)\n",
    "phrase_decoded = phraser.decode(arr)\n",
    "phrase_decoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ________________________________________________________________________________\n",
    "# Generate sequences based on question/answer pairs in the dataset (using phraser)\n",
    "# - x: input sequence (question)\n",
    "# - y: output sequence (answer)\n",
    "questions = np.zeros((len(dataset.text), phraser.max_len, vocab.size), dtype=\"int32\")\n",
    "answers = np.zeros((len(dataset.text), phraser.max_len, vocab.size), dtype=\"int32\")\n",
    "\n",
    "\n",
    "for i, phrase in enumerate(dataset.text):\n",
    "    # split line into question and answer\n",
    "    q, a = phrase.strip().split(\"\\t\")\n",
    "\n",
    "    # encode question and answer (from string to list of integers)\n",
    "    q_encoded = phraser.encode(q)\n",
    "    a_encoded = phraser.encode(a)\n",
    "\n",
    "    # add the encoded question and answer to the dataset\n",
    "    questions[i] = q_encoded\n",
    "    answers[i]   = a_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3725, 50, 37), (3725, 50, 37))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.shape, answers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded question:  [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Encoded answer:  [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Decoded question:  hi how are you doing                              \n",
      "Decoded answer:  im fine how about yourself                        \n"
     ]
    }
   ],
   "source": [
    "# showcase\n",
    "print(\"Encoded question: \", questions[0])\n",
    "print(\"Encoded answer: \", answers[0])\n",
    "\n",
    "print(\"Decoded question: \", phraser.decode(questions[0]))\n",
    "print(\"Decoded answer: \", phraser.decode(answers[0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 10\n",
      "Vectorization...\n",
      "x ->  [[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "y ->  [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the model and dataset.\n",
    "class Config:\n",
    "    # constructor\n",
    "    def __init__(self):\n",
    "        self.training_size:int = 50000\n",
    "        self.digits:int = 5\n",
    "        self.hidden_size:int = 128\n",
    "        self.batch_size:int = 128\n",
    "\n",
    "config = Config()\n",
    "config.training_size = 10\n",
    "config.digits = 5\n",
    "config.hidden_size = 128\n",
    "config.batch_size = 128\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "maxlen = config.digits + 1 + config.digits\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789+- '\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < config.training_size:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, config.digits + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = '{}-{}'.format(a, b)\n",
    "    query = q + ' ' * (maxlen - len(q))\n",
    "    ans = str(a - b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += ' ' * (config.digits + 1 - len(ans))\n",
    "\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "    \n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), maxlen, len(chars)))\n",
    "y = np.zeros((len(questions), config.digits + 1, len(chars)))\n",
    "\n",
    "print(\"x -> \", x)\n",
    "print(\"y -> \", y)\n",
    "\n",
    "# for i, sentence in enumerate(questions):\n",
    "#     x[i] = ctable.encode(sentence, maxlen)\n",
    "# for i, sentence in enumerate(expected):\n",
    "#     y[i] = ctable.encode(sentence, config.digits + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.zeros((3, 4, 5))\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SETS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "split = int(len(x_format) * 0.8)\n",
    "x_train, x_test = x_format[:split], x_format[split:]\n",
    "y_train, y_test = y_format[:split], y_format[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape (train, test):  (2980, 50, 55) (745, 50, 55)\n",
      "y shape (train, test):  (2980, 50, 55) (745, 50, 55)\n"
     ]
    }
   ],
   "source": [
    "print(\"x shape (train, test): \", x_train.shape, x_test.shape)\n",
    "print(\"y shape (train, test): \", y_train.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(MAX_LEN, VOCAB_SIZE)))\n",
    "model.add(Dense(VOCAB_SIZE, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
